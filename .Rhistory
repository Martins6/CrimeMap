caption = 'source: Yahoo Finance')
ptly %>% ggplotly()
# Vizualising
## VaR
ptly <- dt %>%
drop_na() %>%
mutate(Vol = Vol_vec,
VaR = LaggedPrice * VaR_vec,
ES = LaggedPrice * ES_vec,
VaR_price = price + VaR,
ES_price = price + ES) %>%
ggplot(aes(x = Data)) +
geom_path(aes(y = price)) +
#geom_path(aes(y = Vol, colour = 'Vol')) +
geom_path(aes(y = VaR_price, colour = 'VP')) +
geom_path(aes(y = ES_price, colour = 'EP')) +
scale_colour_manual(name = '',
values = c( VP = 'red',
EP = 'blue')) +
labs(title = symbol,
subtitle = 'Value-at-Risk + Expectted Shortfall através de GARCH',
caption = 'source: Yahoo Finance')
ptly %>% ggplotly()
##################### FORECASTING VALUE-AT-RISK + EXPECTED SHORTFALL #######################
##################### ********** GARCH - NORMAL DISTRIB. ###################
p <- 0.05
g = garchFit(~garch(1,1), dt$PRet,
cond.dist = "norm", include.mean = FALSE, trace = FALSE)
# compute sigma for t + 1
sigma. = predict(g)[1,3]
# Computing the white noise from the schock from the GARCH(1,1)
# If our model is right it is supposed to
# follow the distribution from the 'cond.dist' in garchFit
epsilon.t <- (g@residuals/g@sigma.t)
# Compute VAR forecast
`VaR Previsto` = -sigma. * qnorm(p)
# Compute ES forecast under normality
`ES Previsto` = - (sigma.^2) * dnorm(-`VaR Previsto`, sd = sigma.) / p
# # Compute the fitted standard deviation series
Vol_vec <- g %>% fBasics::volatility()
VaR_vec <- Vol_vec * qnorm(p)
ES_vec <- - (Vol_vec^2) * dnorm(VaR_vec, sd = Vol_vec) / p
# Computing the next step forecast
## Data
`Data Prevista` <- dt$Data %>%
last(.) + 1
# Vizualising
## VaR
ply <- dt %>%
# Adding our model to the data matrix
mutate(Vol = Vol_vec,
VaR = -VaR_vec,
ES = ES_vec) %>%
ggplot(aes(x = Data)) +
geom_path(aes(y = PRet)) +
geom_path(aes(y = Vol, colour = 'Volatility')) +
geom_path(aes(y = -VaR, colour = 'Value-at-Risk')) +
geom_path(aes(y = ES, colour = 'Expected-Shortfall')) +
geom_point(aes(x = `Data Prevista`, y = -`VaR Previsto`, colour = 'VaR Previsto')) +
geom_point(aes(x = `Data Prevista`, y = `ES Previsto`, colour = 'ES Previsto')) +
scale_colour_manual(name = '',
values = c(`Value-at-Risk` = 'red',
`Expected-Shortfall` = 'blue',
`VaR Previsto` = 'darkred',
`ES Previsto` =  'darkblue',
Volatility = 'purple')) +
labs(title = symbol,
subtitle = 'Previsão de Value-at-Risk através de GARCH',
caption = 'source: Yahoo Finance') +
theme_bw()
ggplotly(ply)
##################### FORECASTING VALUE-AT-RISK + EXPECTED SHORTFALL #######################
##################### ********** GARCH - NORMAL DISTRIB. ###################
p <- 0.05
g = garchFit(~garch(1,1), dt$PRet,
cond.dist = "norm", include.mean = FALSE, trace = FALSE)
# compute sigma for t + 1
sigma. = predict(g)[1,3]
# Computing the white noise from the schock from the GARCH(1,1)
# If our model is right it is supposed to
# follow the distribution from the 'cond.dist' in garchFit
epsilon.t <- (g@residuals/g@sigma.t)
# Compute VAR forecast
`VaR Previsto` = -sigma. * qnorm(p)
VaR_forecast = `VaR Previsto` * last(dt$price)
VaR_forecast_price = last(price) + VaR_forecast
# Compute ES forecast under normality
`ES Previsto` = - (sigma.^2) * dnorm(-`VaR Previsto`, sd = sigma.) / p
##################### FORECASTING VALUE-AT-RISK + EXPECTED SHORTFALL #######################
##################### ********** GARCH - NORMAL DISTRIB. ###################
p <- 0.05
g = garchFit(~garch(1,1), dt$PRet,
cond.dist = "norm", include.mean = FALSE, trace = FALSE)
# compute sigma for t + 1
sigma. = predict(g)[1,3]
# Computing the white noise from the schock from the GARCH(1,1)
# If our model is right it is supposed to
# follow the distribution from the 'cond.dist' in garchFit
epsilon.t <- (g@residuals/g@sigma.t)
# Compute VAR forecast
`VaR Previsto` = -sigma. * qnorm(p)
VaR_forecast = `VaR Previsto` * last(dt$price)
VaR_forecast_price = last(dt$price) + VaR_forecast
# Compute ES forecast under normality
`ES Previsto` = - (sigma.^2) * dnorm(-`VaR Previsto`, sd = sigma.) / p
ES_forecast = `ES Previsto` * last(dt$price)
ES_forecast_price = last(dt$price) + ES_forecast
# # Compute the fitted standard deviation series
Vol_vec <- g %>% fBasics::volatility()
VaR_vec <- Vol_vec * qnorm(p)
ES_vec <- - (Vol_vec^2) * dnorm(VaR_vec, sd = Vol_vec) / p
# Computing the next step forecast
## Data
`Data Prevista` <- dt$Data %>%
last(.) + 1
# Vizualising
## VaR
ply <- dt %>%
# Adding our model to the data matrix
mutate(VaR = VaR_vec * LaggedPrice,
ES = ES_vec * LaggedPrice,
VaR_price = price + VaR,
ES_price = price + ES,
ES_forecast = `ES Previsto` * last(price)) %>%
ggplot(aes(x = Data)) +
geom_path(aes(y = price)) +
geom_path(aes(y = VaR_price, colour = 'Value-at-Risk')) +
geom_path(aes(y = ES_price, colour = 'Expected-Shortfall')) +
geom_point(aes(x = `Data Prevista`, y = VaR_forecast_price, colour = 'VaR Previsto')) +
geom_point(aes(x = `Data Prevista`, y = `ES Previsto`, colour = 'ES Previsto')) +
scale_colour_manual(name = '',
values = c(`Value-at-Risk` = 'red',
`Expected-Shortfall` = 'blue',
`VaR Previsto` = 'darkred',
`ES Previsto` =  'darkblue',
Volatility = 'purple')) +
labs(title = symbol,
subtitle = 'Previsão de Value-at-Risk através de GARCH',
caption = 'source: Yahoo Finance') +
theme_bw()
# Vizualising
## VaR
ply <- dt %>%
# Adding our model to the data matrix
mutate(VaR = VaR_vec * LaggedPrice,
ES = ES_vec * LaggedPrice,
VaR_price = price + VaR,
ES_price = price + ES) %>%
ggplot(aes(x = Data)) +
geom_path(aes(y = price)) +
geom_path(aes(y = VaR_price, colour = 'Value-at-Risk')) +
geom_path(aes(y = ES_price, colour = 'Expected-Shortfall')) +
geom_point(aes(x = `Data Prevista`, y = VaR_forecast_price, colour = 'VaR Previsto')) +
geom_point(aes(x = `Data Prevista`, y = ES_forecast_price, colour = 'ES Previsto')) +
scale_colour_manual(name = '',
values = c(`Value-at-Risk` = 'red',
`Expected-Shortfall` = 'blue',
`VaR Previsto` = 'darkred',
`ES Previsto` =  'darkblue',
Volatility = 'purple')) +
labs(title = symbol,
subtitle = 'Previsão de Value-at-Risk através de GARCH',
caption = 'source: Yahoo Finance') +
theme_bw()
ggplotly(ply)
# EXECUTING THE APP
#setwd("~/Documents/")
## Global
source('sd/global.R')
## UI
source('sd/ui.R')
## Server
source('sd/server.R')
shiny::shinyApp(ui, server)
install.packages(c("aweek", "broom", "class", "colourvalues", "dbplyr", "DescTools", "fda", "ggfortify", "ggpubr", "git2r", "gss", "KernSmooth", "MASS", "modelr", "naniar", "openxlsx", "pillar", "pkgbuild", "raster", "RcppArmadillo", "recipes", "rematch2", "rex", "rlang", "sf", "systemfonts", "tibble", "tidyr", "timetk", "usethis", "withr", "xml2", "zoo"))
install.packages(c("cluster","mvoutlier","pastecs","fPortfolio"), repos = "http://cran.r-project.org")
install.packages('fPortfolio', dependencies = T)
install.packages('Rsocp')
devtools::install_version(c('Rsocp', 'Rnlminb2', 'Rdonlp2'))
packageurl <- "https://cran.r-project.org/src/contrib/Archive/fPortfolio/fPortfolio_3042.83.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
dt %>% sample_n(1000)
# title: Processing all the data and creating maps in the format of a tibble
# author: Adriel Martins
# date: 04/05/20
# ************************************************************************* #
# ***************** Libraries
# Data manipulation
library(tidyverse)
# Spatial Data Manipulation
library(sp)
library(sf)
library(rgdal)
library(spatstat)
library(maptools)
###################### ****************** Reading Data ######################
# GADM Geospatial data from GADM, mapping the whole of Brazil
brazil.sf <- readRDS('data_pre_processed/gadm36_BRA_3_sf.rds')
# Reading Data from the SSP, which was on Kaagle.
crime <- read_csv("data_pre_processed/SSP/BO_2016.csv")
###################### ****************** Data Wrangling ######################
# Filtering our Map of Brazil into the Map of the City of Sao Paulo.
sp.sf <- brazil.sf %>%
filter(NAME_2 == 'São Paulo') %>%
select(NAME_3) %>%
# Renaming
rename(Bairros = NAME_3) %>%
filter(Bairros == 'Se')
# Calculating bounding box for further wrangling
boundbox.sp <- sp.sf$geometry %>% sf::as_Spatial() %>% bbox()
# Let us modify the data as we see fit
# For our purposes, we need the precise location, so let's clean the data without geotag.
crime.t <- crime %>%
# Filtering just for the year at hand
filter(as.character(ANO_BO) == '2016') %>%
# Selecting what we want
select(LATITUDE, LONGITUDE, MES, RUBRICA) %>%
# Just because I prefer lower case letters
`colnames<-`(., str_to_lower(colnames(.))) %>%
# Interpreting as double the lat-long coordinates
mutate(latitude = as.double(latitude),
longitude = as.double(longitude),
# Changing the name of each month to it's text counterpart
mes = lubridate::month(mes, label = T)) %>%
# Dropping observations without lon-lat
drop_na() %>%
# Let us exclude the points that are not in the bounding box of the City of São Paulo
filter(between(latitude, boundbox.sp[2,1], boundbox.sp[2,2]) &
between(longitude, boundbox.sp[1,1], boundbox.sp[1,2])) %>%
# Let us also aggregate crime, by taking out the 'A.I'.
mutate(rubrica = str_replace_all(rubrica, 'A.I.-', ''))
# Selecting what type of crime
crime <- crime.t %>%
filter(rubrica == 'Roubo (art. 157)')
# Let us put our crime dataset into a proper spatial format for point patterns
crime.sp <- crime %>% select(longitude, latitude) %>% SpatialPoints()
#crime.sp@proj4string <- CRS('+proj=longlat +datum=WGS84 +no_defs')
# zone <- 23
# crime.sp <- spTransform(crime.sp, CRS(paste("+proj=utm +zone=",zone,"+datum=WGS84", sep = '')))
crime.ppp <- crime.sp %>% as('ppp')
sp.sp <- sp.sf %>% as_Spatial()
sp.sp@proj4string <- CRS('+proj=longlat +datum=WGS84 +no_defs')
# zone <- 23
# sp.sp <- spTransform(sp.sp, CRS(paste("+proj=utm +zone=",zone,"+datum=WGS84", sep = '')))
# Constructing the following algorithm:
# 1) Create the Window for your data: a square with the neighborhood that you wish to predict.
# 2) Divide each dimensions into h equal parts, creating h^2 squares, each square with s^2 units.
# 3) Check if there is a crime into each square and atrribute one, if there is, and 0, if there isn't.
# Call that variable Z.
# 4) Create the variable C = count of crime into each square / s^2 units
# 5) Fit the logistic model with GLMMM or GLGM to variable Y (assaulted, not assaulted).
# The point object
crime.ppp %>% summary()
# The region object
sp.sp %>% summary()
# How many parts to divide the region?
h <- 100
# Divinding the region into quadrat or 'little squares'
qcount <- quadratcount(crime.ppp, nx = h, ny = h) %>%
as_tibble()
qcount
# adapting the tibble
## Auxiliary fun
decomposing <- function(x){
x %>%
str_replace('\\[', replacement = '') %>%
strsplit(',') %>%
unlist() %>%
first() %>%
as.numeric()
}
dt <- qcount %>%
mutate(y = unlist(lapply(y, decomposing)),
x = unlist(lapply(x, decomposing)),
crime.event = if_else(n > 0,
1,
0))
#   mutate(crime.event = as.factor(crime.event))
#
# dt$y %>% unique()
dt %>% sample_n(1000)
setwd("~/Documents/CrimeMap")
# title: Processing all the data and creating maps in the format of a tibble
# author: Adriel Martins
# date: 04/05/20
# ************************************************************************* #
# ***************** Libraries
# Data manipulation
library(tidyverse)
# Spatial Data Manipulation
library(sp)
library(sf)
library(rgdal)
library(spatstat)
library(maptools)
###################### ****************** Reading Data ######################
# GADM Geospatial data from GADM, mapping the whole of Brazil
brazil.sf <- readRDS('data_pre_processed/gadm36_BRA_3_sf.rds')
# Reading Data from the SSP, which was on Kaagle.
crime <- read_csv("data_pre_processed/SSP/BO_2016.csv")
###################### ****************** Data Wrangling ######################
# Filtering our Map of Brazil into the Map of the City of Sao Paulo.
sp.sf <- brazil.sf %>%
filter(NAME_2 == 'São Paulo') %>%
select(NAME_3) %>%
# Renaming
rename(Bairros = NAME_3) %>%
filter(Bairros == 'Se')
# Calculating bounding box for further wrangling
boundbox.sp <- sp.sf$geometry %>% sf::as_Spatial() %>% bbox()
# Let us modify the data as we see fit
# For our purposes, we need the precise location, so let's clean the data without geotag.
crime.t <- crime %>%
# Filtering just for the year at hand
filter(as.character(ANO_BO) == '2016') %>%
# Selecting what we want
select(LATITUDE, LONGITUDE, MES, RUBRICA) %>%
# Just because I prefer lower case letters
`colnames<-`(., str_to_lower(colnames(.))) %>%
# Interpreting as double the lat-long coordinates
mutate(latitude = as.double(latitude),
longitude = as.double(longitude),
# Changing the name of each month to it's text counterpart
mes = lubridate::month(mes, label = T)) %>%
# Dropping observations without lon-lat
drop_na() %>%
# Let us exclude the points that are not in the bounding box of the City of São Paulo
filter(between(latitude, boundbox.sp[2,1], boundbox.sp[2,2]) &
between(longitude, boundbox.sp[1,1], boundbox.sp[1,2])) %>%
# Let us also aggregate crime, by taking out the 'A.I'.
mutate(rubrica = str_replace_all(rubrica, 'A.I.-', ''))
# Selecting what type of crime
crime <- crime.t %>%
filter(rubrica == 'Roubo (art. 157)')
# Let us put our crime dataset into a proper spatial format for point patterns
crime.sp <- crime %>% select(longitude, latitude) %>% SpatialPoints()
#crime.sp@proj4string <- CRS('+proj=longlat +datum=WGS84 +no_defs')
# zone <- 23
# crime.sp <- spTransform(crime.sp, CRS(paste("+proj=utm +zone=",zone,"+datum=WGS84", sep = '')))
crime.ppp <- crime.sp %>% as('ppp')
sp.sp <- sp.sf %>% as_Spatial()
sp.sp@proj4string <- CRS('+proj=longlat +datum=WGS84 +no_defs')
# zone <- 23
# sp.sp <- spTransform(sp.sp, CRS(paste("+proj=utm +zone=",zone,"+datum=WGS84", sep = '')))
# Constructing the following algorithm:
# 1) Create the Window for your data: a square with the neighborhood that you wish to predict.
# 2) Divide each dimensions into h equal parts, creating h^2 squares, each square with s^2 units.
# 3) Check if there is a crime into each square and atrribute one, if there is, and 0, if there isn't.
# Call that variable Z.
# 4) Create the variable C = count of crime into each square / s^2 units
# 5) Fit the logistic model with GLMMM or GLGM to variable Y (assaulted, not assaulted).
# The point object
crime.ppp %>% summary()
# The region object
sp.sp %>% summary()
# How many parts to divide the region?
h <- 100
# Divinding the region into quadrat or 'little squares'
qcount <- quadratcount(crime.ppp, nx = h, ny = h) %>%
as_tibble()
qcount
# adapting the tibble
## Auxiliary fun
decomposing <- function(x){
x %>%
str_replace('\\[', replacement = '') %>%
strsplit(',') %>%
unlist() %>%
first() %>%
as.numeric()
}
dt <- qcount %>%
mutate(y = unlist(lapply(y, decomposing)),
x = unlist(lapply(x, decomposing)),
crime.event = if_else(n > 0,
1,
0))
#   mutate(crime.event = as.factor(crime.event))
#
# dt$y %>% unique()
dt %>% sample_n(1000)
dt %>% sample_n(1000) %>% .$crime.event %>% hist()
dt %>% sample_n(1000) %>% .$crime.event %>% hist()
dt %>% sample_n(1000) %>% .$crime.event %>% hist()
dt %>% sample_n(1000) %>% .$crime.event %>% hist()
dt %>% sample_n(1000) %>% .$crime.event %>% hist()
dt %>% sample_n(1000) %>% .$crime.event %>% hist()
dt %>% sample_n(1000) %>% .$crime.event %>% hist()
dt %>% sample_n(1000) %>% .$crime.event %>% hist()
dt.aux <- dt %>% sample_n(1000)
# Fitting GLGM or GLMMM in a Spatial Statistics context
fit.glmm <- spaMM::fitme(crime.event ~ 1 + Matern(1|x + y),
data = dt.aux,
family = binomial(link = "logit"),
HLmethod = 'PQL/L')
# Fitting GLGM or GLMMM in a Spatial Statistics context
fit.glmm <- spaMM::fitme(crime.event ~ 1 + Matern(1|x + y),
data = dt.aux,
family = binomial(link = "logit"),
method = 'PQL/L')
summary(fit.glmm)
# Create an empty raster with the same extent and resolution as the bioclimatic layers
latitude_raster <- longitude_raster <- raster(nrows = 1000,
ncols = 1000,
ext = extent(sp.sp))
# Create an empty raster with the same extent and resolution as the bioclimatic layers
latitude_raster <- longitude_raster <- raster::raster(nrows = 1000,
ncols = 1000,
ext = extent(sp.sp))
# Create an empty raster with the same extent and resolution as the bioclimatic layers
latitude_raster <- longitude_raster <- raster::raster(nrows = 1000,
ncols = 1000,
ext = raster::extent(sp.sp))
# Change the values to be latitude and longitude respectively
longitude_raster[] <- coordinates(longitude_raster)[,1]
latitude_raster[] <- coordinates(latitude_raster)[,2]
# Now create a final prediction stack of the 4 variables we need
pred_stack <- stack(longitude_raster,
latitude_raster)
# Now create a final prediction stack of the 4 variables we need
pred_stack <- stack(longitude_raster,
latitude_raster)
plot(pred_stack)
latitude_raster
latitude_raster %>% plot()
longitude_raster %>% plot()
# Create an empty raster with the same extent and resolution as the bioclimatic layers
latitude_raster <- longitude_raster <- raster::raster(nrows = 100,
ncols = 100,
ext = raster::extent(sp.sp))
# Change the values to be latitude and longitude respectively
longitude_raster[] <- coordinates(longitude_raster)[,1]
latitude_raster[] <- coordinates(latitude_raster)[,2]
latitude_raster %>% plot()
longitude_raster %>% plot()
# Now create a final prediction stack of the 4 variables we need
pred_stack <- stack(longitude_raster,
latitude_raster)
?stack
# Now create a final prediction stack of the 4 variables we need
pred_stack <- raster::stack(longitude_raster,
latitude_raster)
# Now create a final prediction stack of the 4 variables we need
pred_stack <- raster::stack(longitude_raster,
latitude_raster)
coef(fit.glmm)
summary(fit.glmm)
# Rename to ensure the names of the raster layers in the stack match those used in the model
names(pred_stack) <- c("x", "y")
plot(pred_stack)
predicted_prevalence_raster <- predict(pred_stack, fit.glmm)
library(raster)
predicted_prevalence_raster <- predict(pred_stack, fit.glmm)
plot(predicted_prevalence_raster)
lines(sp.sp)
predicted_prevalence_raster_oromia <- mask(predicted_prevalence_raster, sp.sp)
plot(predicted_prevalence_raster_oromia)
dt.aux <- dt %>% sample_n(100)
dt.aux$crime.event %>% hist()
dt.aux <- dt %>% sample_n(100)
dt.aux$crime.event %>% hist()
dt.aux <- dt %>% sample_n(100)
dt.aux$crime.event %>% hist()
dt.aux <- dt %>% sample_n(100)
dt.aux$crime.event %>% hist()
dt.aux <- dt %>% sample_n(100)
dt.aux$crime.event %>% hist()
dt.aux <- dt %>% sample_n(100)
dt.aux$crime.event %>% hist()
dt.aux <- dt %>% sample_n(100)
dt.aux$crime.event %>% hist()
dt.aux <- dt %>% sample_n(100)
dt.aux$crime.event %>% hist()
dt.aux <- dt %>% sample_n(100)
dt.aux$crime.event %>% hist()
dt.aux <- dt %>% sample_n(100)
dt.aux$crime.event %>% hist()
# Fitting GLGM or GLMMM in a Spatial Statistics context
fit.glmm <- spaMM::fitme(crime.event ~ 1 + Matern(1|x + y),
data = dt.aux,
family = binomial(link = "logit"),
method = 'PQL/L')
# Create an empty raster with the same extent and resolution as the bioclimatic layers
latitude_raster <- longitude_raster <- raster::raster(nrows = 100,
ncols = 100,
ext = raster::extent(sp.sp))
# Change the values to be latitude and longitude respectively
longitude_raster[] <- coordinates(longitude_raster)[,1]
latitude_raster[] <- coordinates(latitude_raster)[,2]
# Now create a final prediction stack of the 4 variables we need
pred_stack <- raster::stack(longitude_raster,
latitude_raster)
# Rename to ensure the names of the raster layers in the stack match those used in the model
names(pred_stack) <- c("x", "y")
plot(pred_stack)
predicted_prevalence_raster <- predict(pred_stack, fit.glmm)
plot(predicted_prevalence_raster)
lines(sp.sp)
predicted_prevalence_raster_oromia <- mask(predicted_prevalence_raster, sp.sp)
plot(predicted_prevalence_raster_oromia)
setwd("~/Documents/CrimeMap")
# Title: Crime Map
# Author: Adriel Martins
# Date: 08/05/2020
# EXECUTING THE APP
## Global
source('global.R')
## UI
source('ui.R')
## Server
source('server.R')
shiny::shinyApp(ui, server)
